{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "minDALLE",
      "provenance": [],
      "authorship_tag": "ABX9TyNbwV/Ri8MAmw22TeByKjWl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ouhenio/minDALL-E_notebook/blob/main/minDALLE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **minDALL-E**\n",
        "\n",
        "### Generate images from text prompts using [minDALL-E on Conceptual Captions](https://github.com/kakaobrain/minDALL-E), a model created by Saehoon Kim, Sanghun Cho, Chiheon Kim, Doyup Lee, and Woonhyuk Baek.\n",
        "\n",
        "This notebook was created by [Eugenio Herrera](https://github.com/ouhenio) after following the advice given by [woctezuma](https://github.com/woctezuma) in [this issue](https://github.com/kakaobrain/minDALL-E/issues/7).\n"
      ],
      "metadata": {
        "id": "C0kuYEm-Ci27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown #**Install libraries** üèóÔ∏è\n",
        "# @markdown This cell will take a little while because it has to download several libraries.\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "!git clone https://github.com/kakaobrain/minDALL-E.git\n",
        "import sys\n",
        "sys.path.append('./minDALL-E')\n",
        "%pip install -q -r minDALL-E/requirements.txt\n",
        "%pip install -q pytorch-lightning omegaconf einops tokenizers\n",
        "%pip install -q git+https://github.com/openai/CLIP.git\n",
        "!pip install torch==1.8.1 torchvision==0.9.1 torchtext==0.9.1 -f https://download.pytorch.org/whl/cu101/torch_stable.html"
      ],
      "metadata": {
        "cellView": "form",
        "id": "D6HBQNGmOiuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "iuAbHQNpOU0c"
      },
      "outputs": [],
      "source": [
        "#@markdown #**Check GPU type** üïµÔ∏è\n",
        "#@markdown ### Factory reset runtime if you don't have the desired GPU.\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#@markdown V100 = Excellent (*Available only for Colab Pro users*)\n",
        "\n",
        "#@markdown P100 = Very Good\n",
        "\n",
        "#@markdown T4 = Good (*preferred*)\n",
        "\n",
        "#@markdown K80 = Bad\n",
        "\n",
        "#@markdown P4 = (*Not Recommended*) \n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown #**Define necessary functions** üõ†Ô∏è\n",
        "\n",
        "%cd minDALL-E\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import math\n",
        "import argparse\n",
        "import clip\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "from IPython.display import clear_output\n",
        "\n",
        "output = widgets.Output()\n",
        "plot_output = widgets.Output()\n",
        "\n",
        "sys.path.append(os.path.dirname(os.getcwd()))\n",
        "\n",
        "from dalle.models import Dalle\n",
        "from dalle.utils.utils import set_seed, clip_score\n",
        "\n",
        "device = 'cuda:0'\n",
        "model = Dalle.from_pretrained(\"minDALL-E/1.3B\")\n",
        "model_clip, preprocess_clip = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "model_clip.to(device=device)\n",
        "model.to(device=device)\n",
        "\n",
        "def sampling(prompt, top_k, softmax_temperature, seed, num_candidates=96, num_samples_for_display=36):\n",
        "    # Setup\n",
        "    n_row = int(math.sqrt(num_samples_for_display))\n",
        "    n_col = int(math.sqrt(num_samples_for_display))\n",
        "    set_seed(seed)\n",
        "    \n",
        "    # Sampling\n",
        "    images = model.sampling(prompt=prompt,\n",
        "                            top_k=top_k,\n",
        "                            top_p=None,\n",
        "                            softmax_temperature=softmax_temperature,\n",
        "                            num_candidates=num_candidates,\n",
        "                            device=device).cpu().numpy()\n",
        "    images = np.transpose(images, (0, 2, 3, 1))\n",
        "\n",
        "    # CLIP Re-ranking\n",
        "    rank = clip_score(prompt=prompt, images=images, model_clip=model_clip, preprocess_clip=preprocess_clip, device=device)\n",
        "    images = images[rank]\n",
        "    \n",
        "    images = images[:num_samples_for_display]\n",
        "    fig = plt.figure(figsize=(8*n_row, 8*n_col))\n",
        "\n",
        "    for i in range(num_samples_for_display):\n",
        "        ax = fig.add_subplot(n_row, n_col, i+1)\n",
        "        ax.imshow(images[i])\n",
        "        ax.set_axis_off()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "IzKNaGiGZS5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown #**Parameters** ‚úçÔ∏è\n",
        "\n",
        "text = \"a painting of the seashore under a starry night with full moon\"#@param {type:\"string\"}\n",
        "temp = 1#@param {type:\"number\"}\n",
        "seed = 0#@param {type:\"number\"}\n",
        "top_k = 256#@param {type:\"number\"}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "qsuzEvJo-NQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown #**Run model** üöÄ\n",
        "\n",
        "output.clear_output()\n",
        "plot_output.clear_output()\n",
        "with plot_output:\n",
        "    sampling(prompt=text, \n",
        "        top_k=top_k,\n",
        "        softmax_temperature=temp,\n",
        "        seed=seed,\n",
        "        num_candidates=16,\n",
        "        num_samples_for_display=16)\n",
        "\n",
        "display(output)\n",
        "display(plot_output)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "KFWQLnRuDnWB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}